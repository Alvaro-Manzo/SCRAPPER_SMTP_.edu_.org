import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import sys
import time
import random
import os
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class ScrapperConsola:
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.universidades = [
            'https://www.harvard.edu',
            'https://www.mit.edu', 
            'https://www.stanford.edu',
            'https://www.berkeley.edu',
            'https://www.yale.edu',
            'https://www.princeton.edu',
            'https://www.columbia.edu',
            'https://www.cornell.edu',
            'https://www.upenn.edu',
            'https://www.caltech.edu'
        ]
        
        self.organizaciones = [
            'https://www.redcross.org',
            'https://www.unicef.org',
            'https://www.who.int',
            'https://www.amnesty.org',
            'https://www.greenpeace.org',
            'https://www.oxfam.org',
            'https://www.savethechildren.org'
        ]
        
        # Crear directorio de resultados
        self.results_dir = "resultados_scrapping"
        os.makedirs(self.results_dir, exist_ok=True)

    def extraer_emails_edu_org(self, texto):
        """Extrae solo correos que terminen en .edu o .org"""
        patron = r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.(edu|org)"
        emails_completos = set()
        for match in re.finditer(patron, texto, re.IGNORECASE):
            emails_completos.add(match.group(0).lower())
        return emails_completos

    def crear_session_protegida(self):
        """Crea sesi√≥n HTTP con protecciones"""
        session = requests.Session()
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        session.headers.update(headers)
        return session

    def guardar_resultados(self, emails, nombre_archivo, info_busqueda):
        """Guarda los resultados en correos.txt con numeraci√≥n consecutiva"""
        archivo_principal = os.path.join(self.results_dir, "correos.txt")
        
        # Leer el archivo existente para mantener la numeraci√≥n consecutiva
        emails_existentes = []
        if os.path.exists(archivo_principal):
            try:
                with open(archivo_principal, 'r', encoding='utf-8') as f:
                    contenido = f.read()
                    # Extraer correos existentes para evitar duplicados
                    for linea in contenido.split('\n'):
                        if '. ' in linea and '@' in linea:
                            email = linea.split('. ', 1)[1].strip()
                            if email and email not in emails_existentes:
                                emails_existentes.append(email)
            except:
                emails_existentes = []
        
        # Filtrar correos nuevos (no duplicados)
        emails_nuevos = []
        for email in sorted(emails):
            if email not in emails_existentes:
                emails_nuevos.append(email)
        
        # Escribir al archivo principal
        with open(archivo_principal, 'a', encoding='utf-8') as f:
            # Si es la primera vez, escribir encabezado
            if not emails_existentes:
                f.write("="*70 + "\n")
                f.write("ü§ñ SCRAPPER PRO - TODOS LOS CORREOS ENCONTRADOS\n")
                f.write("="*70 + "\n\n")
            
            # Agregar nueva b√∫squeda
            f.write(f"\nüìÖ {datetime.now().strftime('%d/%m/%Y %H:%M:%S')} - {info_busqueda.get('tipo', 'B√∫squeda')}\n")
            f.write(f"üåê Fuente: {info_busqueda.get('fuente', 'M√∫ltiples')}\n")
            f.write(f"üìß Correos nuevos encontrados: {len(emails_nuevos)}\n")
            f.write("-" * 50 + "\n")
            
            if emails_nuevos:
                # Continuar numeraci√≥n desde donde se qued√≥
                numero_inicial = len(emails_existentes) + 1
                for i, email in enumerate(emails_nuevos):
                    f.write(f"{numero_inicial + i:4d}. {email}\n")
            else:
                f.write("(No se encontraron correos nuevos)\n")
            
            f.write("\n")
        
        return archivo_principal

    def es_url_valida(self, url, dominio):
        """Verifica si la URL pertenece al dominio especificado"""
        try:
            return urlparse(url).netloc.endswith(dominio)
        except:
            return False

    def obtener_urls(self, html, base_url, dominio):
        """Extrae URLs v√°lidas de la p√°gina HTML"""
        soup = BeautifulSoup(html, "html.parser")
        urls = set()
        for enlace in soup.find_all("a", href=True):
            try:
                url = urljoin(base_url, enlace["href"])
                if self.es_url_valida(url, dominio) and url.startswith(('http://', 'https://')):
                    urls.add(url)
            except:
                continue
        return urls

    def scrapear_sitio(self, url_inicial, max_paginas=30):
        """Funci√≥n principal para scrapear un sitio completo"""
        print(f"\n{'='*70}")
        print("ü§ñ SCRAPPER PRO - B√öSQUEDA EN SITIO ESPEC√çFICO")
        print(f"{'='*70}")
        
        dominio_raiz = urlparse(url_inicial).netloc
        visitados = set()
        pendientes = set([url_inicial])
        encontrados = set()

        print(f"üåê Sitio objetivo: {dominio_raiz}")
        print(f"üìÑ L√≠mite de p√°ginas: {max_paginas}")
        print(f"üîç Buscando correos .edu/.org...")
        print("-" * 50)
        
        session = self.crear_session_protegida()
        inicio = time.time()

        while pendientes and len(visitados) < max_paginas:
            url_actual = pendientes.pop()
            if url_actual in visitados:
                continue
                
            print(f"üìÑ P√°gina {len(visitados)+1:2d}/{max_paginas}: {url_actual[:60]}...")
            
            try:
                # Pausa anti-detecci√≥n
                time.sleep(random.uniform(1, 3))
                
                resp = session.get(url_actual, timeout=15)
                resp.raise_for_status()
                
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)[:50]}...")
                visitados.add(url_actual)
                continue

            # Buscar correos .edu/.org
            emails = self.extraer_emails_edu_org(resp.text)
            nuevos_emails = emails - encontrados
            
            if nuevos_emails:
                print(f"   ‚úÖ {len(nuevos_emails)} nuevos correos encontrados!")
                for email in list(nuevos_emails)[:3]:  # Mostrar m√°ximo 3
                    print(f"      üìß {email}")
                if len(nuevos_emails) > 3:
                    print(f"      ... y {len(nuevos_emails) - 3} m√°s")
            
            encontrados.update(emails)
            
            # Obtener nuevas URLs
            try:
                nuevos = self.obtener_urls(resp.text, url_actual, dominio_raiz)
                pendientes.update(nuevos - visitados)
            except:
                pass
                
            visitados.add(url_actual)

        tiempo_total = time.time() - inicio
        
        # Mostrar resultados finales
        print(f"\n{'='*70}")
        print("üìä RESULTADOS FINALES")
        print(f"{'='*70}")
        
        if encontrados:
            print(f"‚úÖ {len(encontrados)} correos .edu/.org encontrados")
            print(f"üåê {len(visitados)} p√°ginas procesadas")
            print(f"‚è±Ô∏è Tiempo total: {tiempo_total:.1f} segundos")
            
            # Guardar archivo
            info = {
                'tipo': 'Sitio Espec√≠fico',
                'fuente': url_inicial,
                'paginas': len(visitados),
                'tiempo': f"{tiempo_total:.1f} segundos"
            }
            
            archivo = self.guardar_resultados(encontrados, "sitio_especifico", info)
            print(f"üìÅ Archivo guardado: {archivo}")
            
            # Mostrar muestra de correos
            print(f"\nüìã MUESTRA DE CORREOS (primeros 10):")
            for i, email in enumerate(sorted(encontrados)[:10], 1):
                print(f"   {i:2d}. {email}")
            
            if len(encontrados) > 10:
                print(f"   ... y {len(encontrados) - 10} m√°s en el archivo")
                
        else:
            print("‚ùå No se encontraron correos .edu/.org")
            print(f"üåê {len(visitados)} p√°ginas procesadas")
            print(f"‚è±Ô∏è Tiempo total: {tiempo_total:.1f} segundos")
            
            print("\nüí° Posibles razones:")
            print("   ‚Ä¢ El sitio no tiene correos p√∫blicos")
            print("   ‚Ä¢ Los correos requieren JavaScript")
            print("   ‚Ä¢ El sitio bloquea bots")
        
        return encontrados

    def buscar_en_lista_sitios(self, urls, nombre_categoria, nombre_archivo):
        """Busca correos en una lista de URLs"""
        print(f"\n{'='*70}")
        print(f"ü§ñ SCRAPPER PRO - {nombre_categoria.upper()}")
        print(f"{'='*70}")
        
        todos_los_emails = set()
        inicio = time.time()
        
        print(f"üéØ Analizando {len(urls)} sitios...")
        print(f"üîç Buscando correos .edu/.org...")
        print("-" * 50)
        
        for i, url in enumerate(urls, 1):
            print(f"\nüìç Sitio {i}/{len(urls)}: {url}")
            
            try:
                session = self.crear_session_protegida()
                delay = random.uniform(2, 5)
                print(f"   ‚è≥ Esperando {delay:.1f}s...")
                time.sleep(delay)
                
                resp = session.get(url, timeout=15)
                resp.raise_for_status()
                
                emails = self.extraer_emails_edu_org(resp.text)
                nuevos_emails = emails - todos_los_emails
                
                if nuevos_emails:
                    print(f"   ‚úÖ {len(nuevos_emails)} nuevos correos encontrados:")
                    for email in list(nuevos_emails)[:3]:
                        print(f"      üìß {email}")
                    if len(nuevos_emails) > 3:
                        print(f"      ... y {len(nuevos_emails) - 3} m√°s")
                else:
                    print(f"   ‚ùå No se encontraron correos nuevos")
                
                todos_los_emails.update(emails)
                
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)[:50]}...")
                continue
        
        tiempo_total = time.time() - inicio
        
        # Mostrar resultados finales
        print(f"\n{'='*70}")
        print("üìä RESULTADOS FINALES")
        print(f"{'='*70}")
        
        if todos_los_emails:
            print(f"‚úÖ {len(todos_los_emails)} correos √∫nicos encontrados")
            print(f"üåê {len(urls)} sitios procesados")
            print(f"‚è±Ô∏è Tiempo total: {tiempo_total:.1f} segundos")
            
            # Guardar archivo
            info = {
                'tipo': nombre_categoria,
                'fuente': f"{len(urls)} sitios web",
                'paginas': len(urls),
                'tiempo': f"{tiempo_total:.1f} segundos"
            }
            
            archivo = self.guardar_resultados(todos_los_emails, nombre_archivo, info)
            print(f"üìÅ Archivo guardado: {archivo}")
            
            # Mostrar todos los correos
            print(f"\nüìã TODOS LOS CORREOS ENCONTRADOS:")
            for i, email in enumerate(sorted(todos_los_emails), 1):
                print(f"   {i:3d}. {email}")
                
        else:
            print("‚ùå No se encontraron correos .edu/.org")
            print(f"üåê {len(urls)} sitios procesados")
            print(f"‚è±Ô∏è Tiempo total: {tiempo_total:.1f} segundos")
        
        return todos_los_emails

    def mostrar_menu(self):
        """Muestra el men√∫ principal"""
        print("\n" + "="*70)
        print("ü§ñ SCRAPPER PRO - VERSI√ìN CONSOLA")
        print("="*70)
        print("1. üåê Scrapear sitio espec√≠fico completo")
        print("2. üéì Buscar en universidades famosas")
        print("3. üè¢ Buscar en organizaciones")
        print("4. üîç Buscar en TODAS las fuentes")
        print("5. üéØ Mostrar Google Dorks")
        print("6. üìÅ Ver archivos generados")
        print("7. üõ°Ô∏è Informaci√≥n sobre privacidad")
        print("8. ‚ùì Ayuda y consejos")
        print("0. ‚ùå Salir")
        print("="*70)

    def mostrar_google_dorks(self):
        """Muestra Google Dorks √∫tiles"""
        print(f"\n{'='*70}")
        print("üéØ GOOGLE DORKS PARA CORREOS .EDU/.ORG")
        print(f"{'='*70}")
        
        dorks = [
            'site:.edu "email" OR "mail" OR "@"',
            'site:.org "contact" OR "@"',
            'site:.edu filetype:pdf "@"',
            'site:.org "staff directory"',
            'site:.edu "faculty" "@"',
            'site:.org "board members"',
            'intext:"@*.edu" -inurl:login',
            'intext:"@*.org" -inurl:login',
            'site:.edu "department" contact',
            'site:.org "team" OR "staff"'
        ]
        
        print("\nüí° Copia y pega estos comandos en Google:")
        print("-" * 50)
        for i, dork in enumerate(dorks, 1):
            print(f"{i:2d}. {dork}")
        
        print(f"\nüìù CONSEJOS ADICIONALES:")
        print("‚Ä¢ Reemplaza t√©rminos por 'directory', 'staff', 'faculty'")
        print("‚Ä¢ Usa comillas para b√∫squedas exactas")
        print("‚Ä¢ Combina con nombres de universidades espec√≠ficas")
        print("‚Ä¢ Prueba con 'professor', 'administrator', 'researcher'")

    def ver_archivos_generados(self):
        """Muestra los archivos generados"""
        print(f"\n{'='*70}")
        print("üìÅ ARCHIVOS GENERADOS")
        print(f"{'='*70}")
        
        try:
            archivos = [f for f in os.listdir(self.results_dir) if f.endswith('.txt')]
            archivos.sort(reverse=True)  # M√°s recientes primero
            
            if archivos:
                print(f"\nüìÇ Directorio: {self.results_dir}")
                print(f"üìä Total de archivos: {len(archivos)}")
                print("-" * 50)
                
                for i, archivo in enumerate(archivos[:10], 1):  # Mostrar √∫ltimos 10
                    ruta_completa = os.path.join(self.results_dir, archivo)
                    size = os.path.getsize(ruta_completa)
                    fecha = datetime.fromtimestamp(os.path.getmtime(ruta_completa))
                    
                    print(f"{i:2d}. {archivo}")
                    print(f"    üìÖ {fecha.strftime('%d/%m/%Y %H:%M:%S')}")
                    print(f"    üì¶ {size:,} bytes")
                    print()
                
                if len(archivos) > 10:
                    print(f"... y {len(archivos) - 10} archivos m√°s")
                    
                print(f"üåç Ubicaci√≥n completa: {os.path.abspath(self.results_dir)}")
            else:
                print("\n‚ùå No se han generado archivos a√∫n")
                print("üí° Ejecuta alguna b√∫squeda para generar archivos")
        
        except Exception as e:
            print(f"\n‚ùå Error accediendo a archivos: {str(e)}")

    def mostrar_info_privacidad(self):
        """Informaci√≥n sobre privacidad"""
        print(f"\n{'='*70}")
        print("üõ°Ô∏è INFORMACI√ìN SOBRE PRIVACIDAD Y SEGURIDAD")
        print(f"{'='*70}")
        print("""
‚ùó ¬øSE VE MI IP CUANDO HAGO SCRAPING?
S√ç, cuando haces scraping, los servidores web pueden ver:
‚Ä¢ Tu direcci√≥n IP real
‚Ä¢ Timestamp de cada petici√≥n  
‚Ä¢ Tu User-Agent (navegador simulado)
‚Ä¢ Todas las p√°ginas que visitas

üîí PROTECCIONES IMPLEMENTADAS EN ESTE SCRAPPER:
‚úÖ Rotaci√≥n aleatoria de User-Agents
‚úÖ Delays aleatorios entre peticiones (1-3 segundos)
‚úÖ Headers realistas de navegador
‚úÖ Manejo de errores y reintentos
‚úÖ L√≠mites de p√°ginas por b√∫squeda

üõ°Ô∏è C√ìMO MEJORAR TU ANONIMATO:
1. üåê VPN (M√ÅS RECOMENDADO):
   ‚Ä¢ NordVPN, ExpressVPN, ProtonVPN
   ‚Ä¢ Oculta completamente tu IP
   ‚Ä¢ M√°s seguro que proxies gratuitos

2. üîó Proxies:
   ‚Ä¢ Configurables en el c√≥digo
   ‚Ä¢ Busca proxies en free-proxy-list.net
   ‚Ä¢ Ten cuidado con proxies maliciosos

3. üï∑Ô∏è Red Tor:
   ‚Ä¢ Anonimato m√°ximo pero muy lento
   ‚Ä¢ Algunos sitios lo bloquean

4. üì± Cambiar de red:
   ‚Ä¢ Usar datos m√≥viles
   ‚Ä¢ Conectarse desde diferentes ubicaciones

‚öñÔ∏è CONSIDERACIONES LEGALES:
‚Ä¢ Lee siempre robots.txt de cada sitio
‚Ä¢ Respeta t√©rminos de servicio
‚Ä¢ No sobrecargues servidores
‚Ä¢ Usa solo para prop√≥sitos leg√≠timos
‚Ä¢ Algunos pa√≠ses tienen leyes espec√≠ficas sobre scraping
""")

    def mostrar_ayuda(self):
        """Muestra ayuda y consejos"""
        print(f"\n{'='*70}")
        print("‚ùì AYUDA Y CONSEJOS PARA USAR EL SCRAPPER")
        print(f"{'='*70}")
        print("""
ü§ñ ¬øQU√â HACE ESTE SCRAPPER?
‚Ä¢ Busca correos electr√≥nicos que terminen en .edu y .org
‚Ä¢ Analiza sitios web de universidades y organizaciones
‚Ä¢ Exporta resultados autom√°ticamente a archivos .txt
‚Ä¢ Incluye protecciones anti-detecci√≥n

üéØ MEJORES ESTRATEGIAS:
1. üéì Para Universidades (.edu):
   ‚Ä¢ Busca en p√°ginas de facultad/directorio
   ‚Ä¢ Analiza departamentos acad√©micos
   ‚Ä¢ Revisa p√°ginas de cursos y profesores
   ‚Ä¢ Eventos acad√©micos y conferencias

2. üè¢ Para Organizaciones (.org):
   ‚Ä¢ P√°ginas "About us" / "Our team"
   ‚Ä¢ Directorios de staff
   ‚Ä¢ Informes anuales (si est√°n en HTML)
   ‚Ä¢ P√°ginas de contacto por departamento

üîç CONSEJOS PARA B√öSQUEDAS EXITOSAS:
‚Ä¢ Empieza con sitios grandes y conocidos
‚Ä¢ Los sitios .edu/.org tienden a tener m√°s correos p√∫blicos
‚Ä¢ Algunos sitios requieren JavaScript (no detectables)
‚Ä¢ Las universidades peque√±as a veces tienen m√°s correos visibles
‚Ä¢ Evita sitios que requieren login

üìÅ ARCHIVOS GENERADOS:
‚Ä¢ Se guardan autom√°ticamente en la carpeta 'resultados_scrapping'
‚Ä¢ Incluyen timestamp y estad√≠sticas detalladas
‚Ä¢ Formato .txt f√°cil de abrir en cualquier programa
‚Ä¢ Contienen an√°lisis por dominio y top dominios

üö® PROBLEMAS COMUNES:
‚Ä¢ "Error 403": El sitio bloquea bots
‚Ä¢ "Error timeout": Sitio lento o no disponible
‚Ä¢ "No se encontraron correos": Normal en muchos sitios
‚Ä¢ "Error de conexi√≥n": Verifica tu internet

üí° CONSEJOS PRO:
‚Ä¢ Ejecuta b√∫squedas en horarios de menor tr√°fico
‚Ä¢ Si un sitio falla, int√©ntalo m√°s tarde
‚Ä¢ Combina con Google Dorks para encontrar m√°s sitios
‚Ä¢ Usa VPN para mayor privacidad
‚Ä¢ Guarda los archivos generados como backup
""")

    def run(self):
        """Ejecuta el scrapper en modo consola"""
        print("üöÄ Iniciando Scrapper Pro - Versi√≥n Consola...")
        
        while True:
            try:
                self.mostrar_menu()
                opcion = input("\nüëÜ Elige una opci√≥n: ").strip()
                
                if opcion == "1":
                    url = input("\nüåê URL del sitio (ej: https://harvard.edu): ").strip()
                    if not url.startswith(('http://', 'https://')):
                        url = 'https://' + url
                    
                    max_pags = input("üìÑ M√°ximo p√°ginas (default 30): ").strip()
                    max_pags = int(max_pags) if max_pags.isdigit() else 30
                    
                    self.scrapear_sitio(url, max_pags)
                    
                elif opcion == "2":
                    emails = self.buscar_en_lista_sitios(
                        self.universidades, 
                        "üéì B√∫squeda en Universidades Famosas",
                        "universidades"
                    )
                    
                elif opcion == "3":
                    emails = self.buscar_en_lista_sitios(
                        self.organizaciones,
                        "üè¢ B√∫squeda en Organizaciones",
                        "organizaciones"
                    )
                    
                elif opcion == "4":
                    print("\nüîç B√öSQUEDA MASIVA - Todas las Fuentes")
                    print("‚ö†Ô∏è Esto puede tomar 15-20 minutos")
                    confirmar = input("¬øContinuar? (s/n): ").lower()
                    
                    if confirmar.startswith('s'):
                        todas_fuentes = self.universidades + self.organizaciones
                        emails = self.buscar_en_lista_sitios(
                            todas_fuentes,
                            "üîç B√∫squeda Masiva (Universidades + Organizaciones)",
                            "busqueda_masiva"
                        )
                    else:
                        print("‚ùå B√∫squeda masiva cancelada")
                        
                elif opcion == "5":
                    self.mostrar_google_dorks()
                    
                elif opcion == "6":
                    self.ver_archivos_generados()
                    
                elif opcion == "7":
                    self.mostrar_info_privacidad()
                    
                elif opcion == "8":
                    self.mostrar_ayuda()
                    
                elif opcion == "0":
                    print("\nüëã ¬°Gracias por usar Scrapper Pro!")
                    print("üìÅ Tus archivos est√°n guardados en:", os.path.abspath(self.results_dir))
                    print("‚öñÔ∏è Recuerda usar los datos de forma √©tica y legal")
                    break
                    
                else:
                    print("\n‚ùå Opci√≥n no v√°lida. Intenta de nuevo.")
                
                if opcion != "0":
                    input("\n‚è∏Ô∏è Presiona Enter para continuar...")
                    
            except KeyboardInterrupt:
                print("\n\n‚èπÔ∏è Programa interrumpido por el usuario")
                break
            except Exception as e:
                print(f"\n‚ùå Error inesperado: {str(e)}")
                input("‚è∏Ô∏è Presiona Enter para continuar...")

if __name__ == "__main__":
    scrapper = ScrapperConsola()
    scrapper.run()
